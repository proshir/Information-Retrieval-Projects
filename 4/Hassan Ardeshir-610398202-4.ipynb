{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d411a64-8caf-428d-82ac-dc1f790cb72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "55bfb972-ef1e-4aa4-9f5c-298440614e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(xml_path):\n",
    "    with open(xml_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    documents = []\n",
    "    for doc in soup.find_all('doc'):\n",
    "        doc_id = int(doc.docno.text.strip())\n",
    "        title = doc.title.text.strip() if doc.title else ''\n",
    "        text = doc.text.strip() if doc.text else ''\n",
    "        \n",
    "        full_text = f'{title}\\n{text}'.strip()\n",
    "        \n",
    "        documents += [{'doc_id': doc_id, 'text': full_text}]\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3c99437a-4b59-4ad7-a481-a420cda33287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Proshir-\n",
      "[nltk_data]     Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a89a95f3-c7b7-4a17-8009-76ae59cab747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return \" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "913fa3e2-ecaf-4c0c-be80-73d504e891af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_id': 2, 'text': 'simpl shear flow past a flat plate in an incompress fluid of small viscos 2 simpl shear flow past a flat plate in an incompress fluid of small viscos depart of aeronaut engin renssela polytechn institut troy simpl shear flow past a flat plate in an incompress fluid of small viscos in the studi of viscou flow past a bodi it is usual necessari to consid a curv shock wave emit from the nose or lead edg of the bodi consequ there exist an inviscid rotat flow region between the shock wave and the boundari layer such a situat aris for instanc in the studi of the hyperson viscou flow past a flat plate the situat is somewhat differ from prandtl classic problem in origin problem the inviscid free stream outsid the boundari layer is irrot while in a hyperson problem the inviscid free stream must be consid as rotat the possibl effect of vortic have been recent discuss by ferri and libbi in the present paper the simpl shear flow past a flat plate in a fluid of small viscos is investig it can be shown that thi problem can again be treat by the approxim the onli novel featur be that the free stream ha a constant vortic the discuss here is restrict to incompress steadi flow'}\n"
     ]
    }
   ],
   "source": [
    "xml_path = 'datas/cran.all.1400.xml'\n",
    "documents = read_dataset(xml_path)\n",
    "for i in range(len(documents)):\n",
    "    documents[i]['text'] = preprocess_text(documents[i]['text'])\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d9a197b8-8cc6-4597-9b84-524d0a7904e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_queries(xml_path):\n",
    "    with open(xml_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, features='xml')\n",
    "\n",
    "    queries = []\n",
    "    for top in soup.find_all('top'):\n",
    "        query_id = int(top.num.text.strip())\n",
    "        query_text = top.title.text.strip()\n",
    "\n",
    "        queries.append({'query_id': query_id, 'query_text': query_text})\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "934f2f51-29bc-4319-8759-d5a2c44f4dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query_id': 4, 'query_text': 'what problems of heat conduction in composite slabs have been solved so\\nfar .'}\n"
     ]
    }
   ],
   "source": [
    "queries_path = 'datas/cran.qry.xml'\n",
    "queries = read_queries(queries_path)\n",
    "print(queries[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "152b9811-096f-43d5-8aa8-8679ca31072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_relevance(relevance_path):\n",
    "    relevance = {}\n",
    "    with open(relevance_path, 'r') as file:\n",
    "        for line in file:\n",
    "            topic, iteration, docno, relevancy = map(str.strip, line.split())\n",
    "            topic = int(topic)\n",
    "            iteration = int(iteration)\n",
    "            relevancy = int(relevancy)\n",
    "\n",
    "            if topic not in relevance:\n",
    "                relevance[topic] = {}\n",
    "\n",
    "            relevance[topic][docno] = relevancy\n",
    "\n",
    "    return relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "b29ef56a-0130-410c-87ee-914863228616",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevance_path = 'datas/cranqrel.trec.txt'\n",
    "relevances = read_relevance(relevance_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ce024568-7827-4678-b333-18f2f08b1735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\projects\\ri\\env\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in d:\\projects\\ri\\env\\lib\\site-packages (from scikit-learn) (1.26.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\projects\\ri\\env\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\projects\\ri\\env\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\projects\\ri\\env\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "29391e4f-e244-4249-a830-5623abe456c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "dfc6930c-213d-4b2c-bcb0-f56c504217cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf_scores(docs):\n",
    "    corpus = [doc['text'] for doc in docs]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidf_scores = {}\n",
    "    for i, doc in enumerate(docs):\n",
    "        doc_id = doc['doc_id']\n",
    "        print(docs)\n",
    "        feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
    "        feature_names_doc = [feature_names[index] for index in feature_index]\n",
    "        tfidf_values_doc = tfidf_matrix[i, feature_index].toarray()[0]\n",
    "        tfidf_scores[doc_id] = dict(zip(feature_names_doc, tfidf_values_doc))\n",
    "\n",
    "    return tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "a49b24ce-ad6e-4ce7-90b8-93db861a9862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_scores = compute_tfidf_scores(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "45371209-cd90-4a9c-b1ba-3b7f86d4d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents_with_relevance(query, tfidf_scores, tfidf_vectorizer, relevance_judgments):\n",
    "    query = preprocess_text(query)\n",
    "\n",
    "    print(tfidf_scores)\n",
    "    # Get feature names from tfidf_scores\n",
    "    feature_names = list(tfidf_scores.keys())\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.transform([query])\n",
    "    \n",
    "    similarities = {}\n",
    "    for i, doc_id in enumerate(feature_names):\n",
    "        similarity = (tfidf_matrix * tfidf_matrix[i].reshape(-1, 1)).sum()\n",
    "        similarities[doc_id] = similarity\n",
    "\n",
    "    ranked_docs = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    ranked_docs_with_relevance = [(doc_id, similarity, relevance_judgments[doc_id]) for doc_id, similarity in ranked_docs]\n",
    "    print(feature_names)\n",
    "    return ranked_docs_with_relevance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a25128a0-5362-4118-9c59-38c5664d41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_texts = [doc['text'] for doc in documents]\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "6c797890-3870-4664-a58e-3e458afefaf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "row index (1) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[272], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m query_id \u001b[38;5;241m=\u001b[39m query_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m query_text \u001b[38;5;241m=\u001b[39m query_info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m ranked_documents_with_relevance \u001b[38;5;241m=\u001b[39m \u001b[43mrank_documents_with_relevance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfidf_vectorizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelevances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResults for Query \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(query_id, query_text))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rank, (doc_id, similarity, relevance) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(ranked_documents_with_relevance, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "Cell \u001b[1;32mIn[269], line 12\u001b[0m, in \u001b[0;36mrank_documents_with_relevance\u001b[1;34m(query, tfidf_scores, tfidf_vectorizer, relevance_judgments)\u001b[0m\n\u001b[0;32m     10\u001b[0m similarities \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(feature_names):\n\u001b[1;32m---> 12\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m (tfidf_matrix \u001b[38;5;241m*\u001b[39m \u001b[43mtfidf_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     13\u001b[0m     similarities[doc_id] \u001b[38;5;241m=\u001b[39m similarity\n\u001b[0;32m     15\u001b[0m ranked_docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(similarities\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mD:\\projects\\RI\\env\\Lib\\site-packages\\scipy\\sparse\\_index.py:44\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m---> 44\u001b[0m     row, col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Dispatch to specialized methods.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row, INT_TYPES):\n",
      "File \u001b[1;32mD:\\projects\\RI\\env\\Lib\\site-packages\\scipy\\sparse\\_index.py:152\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    150\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mM \u001b[38;5;129;01mor\u001b[39;00m row \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m M:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow index (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) out of range\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m row)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    154\u001b[0m     row \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m M\n",
      "\u001b[1;31mIndexError\u001b[0m: row index (1) out of range"
     ]
    }
   ],
   "source": [
    "for query_info in queries:\n",
    "    query_id = query_info['query_id']\n",
    "    query_text = query_info['query_text']\n",
    "\n",
    "    ranked_documents_with_relevance = rank_documents_with_relevance(query_text, tfidf_scores, tfidf_vectorizer, relevances)\n",
    "\n",
    "    print(\"Results for Query {}: {}\".format(query_id, query_text))\n",
    "    for rank, (doc_id, similarity, relevance) in enumerate(ranked_documents_with_relevance, start=1):\n",
    "        print(\"Rank {}: Document ID: {}, Similarity: {}, Relevance: {}\".format(rank, doc_id, similarity, relevance))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e299760-0c42-4c6d-a540-f68763407571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df1889e-8158-40a6-972f-92179c58f2fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
